{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Local\\Miniconda\\envs\\Pytorch2\\Lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n",
      "C:\\Users\\Kaefer\\AppData\\Local\\Temp\\ipykernel_24180\\1116110341.py:14: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, ToTensor, Resize, Normalize, CenterCrop\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "from pylab import savefig\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from collections import OrderedDict\n",
    "\n",
    "#num of epochs\n",
    "max_epochs = 1\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 10\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_size = 64\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 100\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002\n",
    "\n",
    "# Beta1 hyperparameter for Adam optimizers\n",
    "b1 = 0.5\n",
    "b2 = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*8) x 4 x 4``\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*4) x 8 x 8``\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*2) x 16 x 16``\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf) x 32 x 32``\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh())\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is ``(nc) x 64 x 64``\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. ``(ndf) x 32 x 32``\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. ``(ndf*2) x 16 x 16``\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. ``(ndf*4) x 8 x 8``\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. ``(ndf*8) x 4 x 4``\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        # self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # networks\n",
    "        self.generator = Generator()\n",
    "        self.discriminator = Discriminator()\n",
    "\n",
    "        self.validation_z = torch.randn(8, nz)\n",
    "\n",
    "        self.example_input_array = torch.zeros(2, nz)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "    def adversarial_loss(self, y_hat, y):\n",
    "        return F.binary_cross_entropy(y_hat, y)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        imgs, _ = batch\n",
    "\n",
    "        # sample noise\n",
    "        z = torch.randn(imgs.shape[0], nz)\n",
    "        z = z.type_as(imgs)\n",
    "\n",
    "        # train generator\n",
    "        if optimizer_idx == 0:\n",
    "\n",
    "            # generate images\n",
    "            self.generated_imgs = self(z)\n",
    "\n",
    "            # log sampled images\n",
    "            sample_imgs = self.generated_imgs[:6]\n",
    "            grid = torchvision.utils.make_grid(sample_imgs)\n",
    "            self.logger.experiment.add_image('generated_images', grid, 0)\n",
    "\n",
    "            # ground truth result (ie: all fake)\n",
    "            # put on GPU because we created this tensor inside training_loop\n",
    "            valid = torch.ones(imgs.size(0), 1)\n",
    "            valid = valid.type_as(imgs)\n",
    "\n",
    "            # adversarial loss is binary cross-entropy\n",
    "            g_loss = self.adversarial_loss(self.discriminator(self(z)), valid)\n",
    "            tqdm_dict = {'g_loss': g_loss}\n",
    "            output = OrderedDict({\n",
    "                'loss': g_loss,\n",
    "                'progress_bar': tqdm_dict,\n",
    "                'log': tqdm_dict\n",
    "            })\n",
    "            return output\n",
    "\n",
    "        # train discriminator\n",
    "        if optimizer_idx == 1:\n",
    "            # Measure discriminator's ability to classify real from generated samples\n",
    "\n",
    "            # how well can it label as real?\n",
    "            valid = torch.ones(imgs.size(0), 1)\n",
    "            valid = valid.type_as(imgs)\n",
    "\n",
    "            real_loss = self.adversarial_loss(self.discriminator(imgs), valid)\n",
    "\n",
    "            # how well can it label as fake?\n",
    "            fake = torch.zeros(imgs.size(0), 1)\n",
    "            fake = fake.type_as(imgs)\n",
    "\n",
    "            fake_loss = self.adversarial_loss(\n",
    "                self.discriminator(self(z).detach()), fake)\n",
    "\n",
    "            # discriminator loss is the average of these\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            tqdm_dict = {'d_loss': d_loss}\n",
    "            output = OrderedDict({\n",
    "                'loss': d_loss,\n",
    "                'progress_bar': tqdm_dict,\n",
    "                'log': tqdm_dict\n",
    "            })\n",
    "            return output\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # lr = self.lr\n",
    "        # b1 = self.b1\n",
    "        # b2 = self.b2\n",
    "        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "        return [opt_g, opt_d], []\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        transform=Compose([\n",
    "            Resize(image_size),\n",
    "            CenterCrop(image_size),\n",
    "            ToTensor(),\n",
    "            Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])\n",
    "\n",
    "        dataset = torchvision.datasets.ImageFolder(\n",
    "            root='./CelebA_data',\n",
    "            transform=transform)\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    def validation_step(self):\n",
    "        z = self.validation_z #.to(self.device)\n",
    "\n",
    "        # log sampled images\n",
    "        sample_imgs = self(z)\n",
    "        grid = torchvision.utils.make_grid(sample_imgs)\n",
    "        self.logger.experiment.add_image('generated_images', grid, self.current_epoch)\n",
    "\n",
    "    def test_step(self):\n",
    "        z = self.validation_z #.to(self.device)\n",
    "\n",
    "        # log sampled images\n",
    "        sample_imgs = self(z)\n",
    "        grid = torchvision.utils.make_grid(sample_imgs)\n",
    "        self.logger.experiment.add_image('generated_images', grid, self.current_epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class load_data(pl.LightningDataModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        transform = Compose([\n",
    "            Resize(image_size),\n",
    "            CenterCrop(image_size),\n",
    "            ToTensor(),\n",
    "            Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])\n",
    "\n",
    "        self.train = torchvision.datasets.CelebA(\n",
    "            root='./CelebA_data',\n",
    "            split='train',\n",
    "            download=True,\n",
    "            transform=transform)\n",
    "        \n",
    "        print('Data loaded')\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The daily quota of the file img_align_celeba.zip is exceeded and it can't be downloaded. This is a limitation of Google Drive and can only be overcome by trying again later.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Data\\COMP3710\\GAN-CelebA\\GAN - CelebA.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Data/COMP3710/GAN-CelebA/GAN%20-%20CelebA.ipynb#W3sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     plot \u001b[39m=\u001b[39m sn\u001b[39m.\u001b[39mrelplot(data\u001b[39m=\u001b[39mmetrics, kind\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mline\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Data/COMP3710/GAN-CelebA/GAN%20-%20CelebA.ipynb#W3sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     plot\u001b[39m.\u001b[39msavefig(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mlog_dir\u001b[39m}\u001b[39;00m\u001b[39m/graph.png\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Data/COMP3710/GAN-CelebA/GAN%20-%20CelebA.ipynb#W3sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m: main()\n",
      "\u001b[1;32mc:\\Data\\COMP3710\\GAN-CelebA\\GAN - CelebA.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Data/COMP3710/GAN-CelebA/GAN%20-%20CelebA.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m():\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Data/COMP3710/GAN-CelebA/GAN%20-%20CelebA.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     mod \u001b[39m=\u001b[39m GAN()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Data/COMP3710/GAN-CelebA/GAN%20-%20CelebA.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     data \u001b[39m=\u001b[39m load_data()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Data/COMP3710/GAN-CelebA/GAN%20-%20CelebA.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m      \u001b[39m#setup trainer with logging of results\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Data/COMP3710/GAN-CelebA/GAN%20-%20CelebA.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Data/COMP3710/GAN-CelebA/GAN%20-%20CelebA.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         max_epochs\u001b[39m=\u001b[39mmax_epochs, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Data/COMP3710/GAN-CelebA/GAN%20-%20CelebA.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39m# accelerator='gpu', \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Data/COMP3710/GAN-CelebA/GAN%20-%20CelebA.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39m# devices=1,\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Data/COMP3710/GAN-CelebA/GAN%20-%20CelebA.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         logger\u001b[39m=\u001b[39mCSVLogger(save_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlogs/\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m#save parameters into csv\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Data/COMP3710/GAN-CelebA/GAN%20-%20CelebA.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         callbacks\u001b[39m=\u001b[39m[LearningRateMonitor(logging_interval\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m\"\u001b[39m), TQDMProgressBar(refresh_rate\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)])\n",
      "\u001b[1;32mc:\\Data\\COMP3710\\GAN-CelebA\\GAN - CelebA.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Data/COMP3710/GAN-CelebA/GAN%20-%20CelebA.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Data/COMP3710/GAN-CelebA/GAN%20-%20CelebA.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m transform \u001b[39m=\u001b[39m Compose([\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Data/COMP3710/GAN-CelebA/GAN%20-%20CelebA.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     Resize(image_size),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Data/COMP3710/GAN-CelebA/GAN%20-%20CelebA.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     CenterCrop(image_size),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Data/COMP3710/GAN-CelebA/GAN%20-%20CelebA.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     ToTensor(),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Data/COMP3710/GAN-CelebA/GAN%20-%20CelebA.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     Normalize((\u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m), (\u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m)),])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Data/COMP3710/GAN-CelebA/GAN%20-%20CelebA.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39;49mdatasets\u001b[39m.\u001b[39;49mCelebA(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Data/COMP3710/GAN-CelebA/GAN%20-%20CelebA.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     root\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./CelebA_data\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Data/COMP3710/GAN-CelebA/GAN%20-%20CelebA.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     split\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Data/COMP3710/GAN-CelebA/GAN%20-%20CelebA.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     download\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Data/COMP3710/GAN-CelebA/GAN%20-%20CelebA.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     transform\u001b[39m=\u001b[39;49mtransform)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Data/COMP3710/GAN-CelebA/GAN%20-%20CelebA.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mData loaded\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Local\\Miniconda\\envs\\Pytorch2\\Lib\\site-packages\\torchvision\\datasets\\celeba.py:80\u001b[0m, in \u001b[0;36mCelebA.__init__\u001b[1;34m(self, root, split, target_type, transform, target_transform, download)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mtarget_transform is specified but target_type is empty\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     79\u001b[0m \u001b[39mif\u001b[39;00m download:\n\u001b[1;32m---> 80\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload()\n\u001b[0;32m     82\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_integrity():\n\u001b[0;32m     83\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Local\\Miniconda\\envs\\Pytorch2\\Lib\\site-packages\\torchvision\\datasets\\celeba.py:150\u001b[0m, in \u001b[0;36mCelebA.download\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mfor\u001b[39;00m (file_id, md5, filename) \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_list:\n\u001b[1;32m--> 150\u001b[0m     download_file_from_google_drive(file_id, os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_folder), filename, md5)\n\u001b[0;32m    152\u001b[0m extract_archive(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_folder, \u001b[39m\"\u001b[39m\u001b[39mimg_align_celeba.zip\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Local\\Miniconda\\envs\\Pytorch2\\Lib\\site-packages\\torchvision\\datasets\\utils.py:246\u001b[0m, in \u001b[0;36mdownload_file_from_google_drive\u001b[1;34m(file_id, root, filename, md5)\u001b[0m\n\u001b[0;32m    243\u001b[0m         api_response, content \u001b[39m=\u001b[39m _extract_gdrive_api_response(response)\n\u001b[0;32m    245\u001b[0m     \u001b[39mif\u001b[39;00m api_response \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mQuota exceeded\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 246\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    247\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe daily quota of the file \u001b[39m\u001b[39m{\u001b[39;00mfilename\u001b[39m}\u001b[39;00m\u001b[39m is exceeded and it \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be downloaded. This is a limitation of Google Drive \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    249\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mand can only be overcome by trying again later.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m         )\n\u001b[0;32m    252\u001b[0m     _save_response_content(content, fpath)\n\u001b[0;32m    254\u001b[0m \u001b[39m# In case we deal with an unhandled GDrive API response, the file should be smaller than 10kB and contain only text\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The daily quota of the file img_align_celeba.zip is exceeded and it can't be downloaded. This is a limitation of Google Drive and can only be overcome by trying again later."
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    mod = GAN()\n",
    "    data = load_data()\n",
    "\n",
    "     #setup trainer with logging of results\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs, \n",
    "        # accelerator='gpu', \n",
    "        # devices=1,\n",
    "        logger=CSVLogger(save_dir=\"logs/\"), #save parameters into csv\n",
    "        callbacks=[LearningRateMonitor(logging_interval=\"step\"), TQDMProgressBar(refresh_rate=10)])\n",
    "    \n",
    "    #train then test module\n",
    "    trainer.fit(mod)\n",
    "    # trainer.test(mod)\n",
    "\n",
    "    #get metrics for plotting lr, loss, acc\n",
    "    metrics = pd.read_csv(f\"{trainer.logger.log_dir}/metrics.csv\")\n",
    "    del metrics[\"step\"]\n",
    "    metrics.set_index(\"epoch\", inplace=True)\n",
    "    display(metrics.dropna(axis=1, how=\"all\").head())\n",
    "    plot = sn.relplot(data=metrics, kind=\"line\")\n",
    "    plot.savefig(f\"{trainer.logger.log_dir}/graph.png\")\n",
    "     \n",
    "\n",
    "if __name__ == '__main__': main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
